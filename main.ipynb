{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3408409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from collections import defaultdict\n",
    "from Transaction import Transaction\n",
    "from operator import add\n",
    "from pEFIM import pEFIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ee1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultvalue():\n",
    "    return []\n",
    "\n",
    "# a dictionary that maps the items to its neighbors\n",
    "nbh = defaultdict(defaultvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1047f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/19 18:12:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# variables used in the algorithm\n",
    "APP_NAME = \"PSHUIM\"\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = 'thesisDatabase.txt'\n",
    "nbhfile = 'neighbor.txt'\n",
    "numPartitions = 4\n",
    "minUtil = 50\n",
    "partitionType = 'lookup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7200c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a neighborhood dictionary from neighborhood file\n",
    "def BuildNeighborhoodFile(file):\n",
    "    global nbh\n",
    "    f = open(file, 'r')\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        line = line.strip().split(' ')\n",
    "        neighbors = [int(x) for x in line[1:]]\n",
    "        nbh[int(line[0])] = set(neighbors)\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfc6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTransaction(line):\n",
    "#     gets a input line and builds a transaction with the line\n",
    "    line = line.strip().split(':')\n",
    "    items = line[0].strip().split(' ')\n",
    "    items = [int(item) for item in items]\n",
    "    twu = float(line[1])\n",
    "    utilities = line[2].strip().split(' ')\n",
    "    utilities = [float(utility) for utility in utilities]\n",
    "    pmus = line[3].strip().split(' ')\n",
    "    pmus = [float(pmu) for pmu in pmus]\n",
    "    # creating a transaction\n",
    "    transaction = Transaction(items, utilities, twu, pmus)\n",
    "    return transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01334a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileStats(transactions):\n",
    "    transactionUtilities = transactions.flatMap(lambda x: [x.getTransactionUtility()])\n",
    "    totalutility = transactionUtilities.reduce(add)\n",
    "    datasetLen = len(transactionUtilities.collect())\n",
    "    return {\n",
    "        'len' : datasetLen,\n",
    "        'totalUtility' : totalutility\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a33161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function not only revises the transaction but also calculates the NSTU value of each secondary item\n",
    "def reviseTransactions(transaction):\n",
    "    transaction.removeUnpromisingItems(oldNamesToNewNames_broadcast.value)\n",
    "    return transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fa5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the neighborhood subtree utility of secondary items\n",
    "def calculateNSTUFirstTime(transaction):\n",
    "    # secondary items\n",
    "    secondaryItems = list(oldNamesToNewNames_broadcast.value.keys())\n",
    "    items = transaction.getItems()\n",
    "    utilities = transaction.getUtilities()\n",
    "    itemsUtilityList = []\n",
    "    for idx, item in enumerate(items):\n",
    "        i = idx + 1\n",
    "        sum_utility = utilities[idx] \n",
    "        while(i < len(items)):\n",
    "            if items[i] in nbhNew_broadcast.value[item]:\n",
    "                sum_utility += utilities[i]\n",
    "            i += 1\n",
    "        itemsUtilityList.append((item, sum_utility))\n",
    "    return itemsUtilityList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba26df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function just collects the transaction and prints the items and utilities present in the transaction\n",
    "def printTransactions(transactions):\n",
    "    for transaction in transactions.collect():\n",
    "        print('transaction start')\n",
    "        print(transaction.getItems())\n",
    "        print(transaction.getUtilities())\n",
    "        print('transaction ends') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfbc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the items between the partitions based on certain techniques\n",
    "def divideItems(items, numPartitions, partitionType):\n",
    "    itemNode = {}\n",
    "    NodeToItemMap = {}\n",
    "    for i in range(numPartitions):\n",
    "        NodeToItemMap[i] = []\n",
    "    if partitionType == 'lookup':\n",
    "        i = 0\n",
    "        inc = 1\n",
    "        flag = False\n",
    "        for item in items:\n",
    "            itemNode[item] = i\n",
    "            NodeToItemMap[i].append(item)\n",
    "            i += inc\n",
    "            if (i == 0) or (i == numPartitions -1):\n",
    "                if flag:\n",
    "                    if i == 0:\n",
    "                        inc = 1\n",
    "                    else:\n",
    "                        inc = -1\n",
    "                    flag = False\n",
    "                else:\n",
    "                    inc = 0\n",
    "                    flag = True\n",
    "    for i in range(numPartitions):\n",
    "        NodeToItemMap[i] = set(NodeToItemMap[i])\n",
    "        \n",
    "    return itemNode, NodeToItemMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16e371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultBooleanValue():\n",
    "    return False\n",
    "\n",
    "def mapTransaction(transaction):\n",
    "    items = transaction.getItems()\n",
    "    utilities = transaction.getUtilities()\n",
    "    totalUtility = transaction.getTransactionUtility()\n",
    "    mapItemToNodeID = itemToNodeMap_broadcast.value\n",
    "    mapNodeID = defaultdict(defaultBooleanValue)\n",
    "    transactionList = []\n",
    "    cumulativeUtility = 0\n",
    "    primaryItems = list(mapItemToNodeID.keys())\n",
    "    for idx, item in enumerate(items):\n",
    "        if item not in primaryItems:\n",
    "            cumulativeUtility += utilities[idx]\n",
    "            continue\n",
    "        nodeID = mapItemToNodeID[item]\n",
    "        # if this transaction is not assigned to the node \n",
    "        if not mapNodeID[nodeID]:\n",
    "            # create a new transaction\n",
    "            newTransaction = Transaction(items[idx:], utilities[idx:], totalUtility - cumulativeUtility)\n",
    "            transactionList.append((nodeID, newTransaction))\n",
    "            mapNodeID[nodeID] = True\n",
    "        cumulativeUtility += utilities[idx]\n",
    "    return transactionList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ccb0752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BuildNeighborhoodFile(nbhfile)\n",
    "\n",
    "# reading the data from the text file and transorfming each line into a transaction\n",
    "transactions = sc.textFile(inputfile, numPartitions).map(lambda x : buildTransaction(x))\n",
    "transactions.persist()\n",
    "\n",
    "# compute the statistics of the database\n",
    "filestats = getFileStats(transactions)\n",
    "\n",
    "# calculate the pmu value for each item present in the database\n",
    "pmuDict = dict(transactions.flatMap(lambda x: [item for item in zip(x.getItems(), x.getPmus())]).reduceByKey(add).filter(lambda x: x[1] >= minUtil).collect())\n",
    "\n",
    "# the keys in the dictionary are the items which we keep in the database we call them as primary items\n",
    "secondaryItems = list(pmuDict.keys())\n",
    "\n",
    "# sorting the primary keys in increasing order of their PMU values\n",
    "secondaryItems.sort(key = lambda x: pmuDict[x])\n",
    "\n",
    "# give new names to the items based upon their ordering starting from 1\n",
    "oldNamesToNewNames = {} # dictionary for storing the mappings from old names to new names\n",
    "newNamesToOldNames = {} # dictionary to map from new names to old names\n",
    "currentName = 1\n",
    "for idx, item in enumerate(secondaryItems):\n",
    "    oldNamesToNewNames[item] = currentName\n",
    "    newNamesToOldNames[currentName] = item\n",
    "    secondaryItems[idx] = currentName\n",
    "    currentName += 1\n",
    "\n",
    "# broadcasting the oldNamesToNewNames Dictionary which will be used by the transaction to get the revised transaction\n",
    "oldNamesToNewNames_broadcast = sc.broadcast(oldNamesToNewNames)\n",
    "newNamesToOldNames_broadcast = sc.broadcast(newNamesToOldNames)\n",
    "nbh_broadcast = sc.broadcast(dict(nbh))\n",
    "minUtil_broadcast = sc.broadcast(minUtil)\n",
    "\n",
    "# convert the items from old names to new names in nbh dictionary\n",
    "nbhNew = {}\n",
    "oldsecondaryItems = list(oldNamesToNewNames.keys())\n",
    "for key in nbh.keys():\n",
    "    if key in oldsecondaryItems:\n",
    "        newNeighbors  = [oldNamesToNewNames[x] for x in nbh[key] if x in oldsecondaryItems]\n",
    "        if len(newNeighbors) != 0:\n",
    "            nbhNew[oldNamesToNewNames[key]] = set(newNeighbors)\n",
    "nbhNew_broadcast = sc.broadcast(nbhNew)\n",
    "\n",
    "# Remove non secondary items from each transaction and sort remaining items in increasing order of their PMU values\n",
    "revisedTransactions = transactions.map(reviseTransactions).filter(lambda x: len(x.getItems()) > 0)\n",
    "revisedTransactions.persist()\n",
    "transactions.unpersist()\n",
    "\n",
    "# Calculate the neighborhood subtree utility of each item in secondary item\n",
    "NSTU_dict = dict(revisedTransactions.flatMap(calculateNSTUFirstTime).reduceByKey(add).filter(lambda x: x[1] >= minUtil).collect())\n",
    "\n",
    "# primary items or the items which need to be projected in DFS traversal of the search space\n",
    "primaryItems = list(NSTU_dict.keys())\n",
    "primaryItems.sort(key= lambda x: pmuDict[newNamesToOldNames[x]])\n",
    "\n",
    "\n",
    "itemToNodeMap, nodeToItemsMap = divideItems(primaryItems, numPartitions, partitionType)\n",
    "itemToNodeMap_broadcast = sc.broadcast(itemToNodeMap)\n",
    "nodeToItemsMap_broadcast = sc.broadcast(dict(nodeToItemsMap))\n",
    "\n",
    "# creating a new key-value RDD where key is node id and value is list of transactions at that node id\n",
    "partitionTransactions = revisedTransactions.flatMap(mapTransaction).groupByKey().mapValues(list)\n",
    "partitionTransactions.persist()\n",
    "revisedTransactions.unpersist()\n",
    "\n",
    "# repartition the data into nodes depending upon the key\n",
    "# transactions = transactions.partitionBy(numPartitions, lambda k: int(k[0]))\n",
    "# partitioner = RangePartitioner(numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c258cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parllelEFIM(nodeData):\n",
    "    currNode = nodeData[0]\n",
    "    transactions = nodeData[1]\n",
    "    primaryItems = nodeToItemsMap_broadcast.value\n",
    "    primaryItems = primaryItems[currNode]\n",
    "    mapItemsToNeighbors = nbh_broadcast.value\n",
    "    minUtil = minUtil_broadcast.value\n",
    "    oldNamesToNewNames = oldNamesToNewNames_broadcast.value\n",
    "    newNamesToOldNames = newNamesToOldNames_broadcast.value\n",
    "    secondaryItems = list(newNamesToOldNames.keys())\n",
    "    pefim = pEFIM(mapItemsToNeighbors, minUtil, primaryItems, secondaryItems, transactions, newNamesToOldNames, oldNamesToNewNames)\n",
    "    output = pefim.runAlgo()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5fd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, transaction in enumerate(partitionTransactions.collect()):\n",
    "#     if idx == 1:\n",
    "#         parllelEFIM(transaction)\n",
    "    \n",
    "huis = partitionTransactions.map(parllelEFIM).groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c3f3db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "itemsets = [y for x in huis[0][1] if len(x) > 0 for y in x]\n",
    "print(len(itemsets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
